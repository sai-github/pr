{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Load all the necessary packages\n",
    "2) Load the data\n",
    "3) Find the reference utterance from each class\n",
    "    Reference utterance is that one whose length is maximum and common to all the class\n",
    "4) For all the utterances..do DTW with the reference utterance of the respective classes and find the path\n",
    "5) Using the indices in the path...shrink or expand your utterance and store it in 'X'\n",
    "5) Once you get the fixed length data..pass it to your SVM library\n",
    "6) Get the required scores and accuracy using different attributes of SVM library\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import euclidean\n",
    "from numpy import array, zeros, argmin, inf, equal, ndim\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set Path to the dataset\n",
    "d1 = glob.glob('C:/Users/iNitesh/Dropbox/Datasets/digit_recognition_speech/data/isolated/36/1/*')\n",
    "d2 = glob.glob('C:/Users/iNitesh/Dropbox/Datasets/digit_recognition_speech/data/isolated/36/2/*')\n",
    "d3 = glob.glob('C:/Users/iNitesh/Dropbox/Datasets/digit_recognition_speech/data/isolated/36/3/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data.....\n",
    "def load_mfcc(list_paths):\n",
    "    X = []\n",
    "    for i in list_paths:\n",
    "        X.append(np.array(pd.read_csv(i, sep = \" \", skiprows = [0], header = None))[:,1:])\n",
    "\n",
    "    return np.array(X)\n",
    "\n",
    "# def combine(X):\n",
    "#     temp_X = []\n",
    "\n",
    "#     for i in X:\n",
    "#         for j in i:\n",
    "#             temp_X.append(j)\n",
    "\n",
    "#     return np.array(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load and structure data\n",
    "X1 = load_mfcc(d1)\n",
    "X2 = load_mfcc(d2)\n",
    "X3 = load_mfcc(d3)\n",
    "\n",
    "# Train Test data partition\n",
    "# X1_train,X1_test = train_test_split(X1, test_size=0.3, random_state=42)\n",
    "# X2_train,X2_test = train_test_split(X2, test_size=0.3, random_state=42)\n",
    "# X3_train,X3_test = train_test_split(X3, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Method to traceback the path generated after DTW\n",
    "def _traceback(D):\n",
    "    i, j = array(D.shape) - 2\n",
    "    p, q = [i], [j]\n",
    "    while ((i > 0) or (j > 0)):\n",
    "        tb = argmin((D[i, j], D[i, j+1], D[i+1, j]))\n",
    "        if (tb == 0):\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif (tb == 1):\n",
    "            i -= 1\n",
    "        else: # (tb == 2):\n",
    "            j -= 1\n",
    "        p.insert(0, i)\n",
    "        q.insert(0, j)\n",
    "    return array(p), array(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for DTW\n",
    "def dtw(x, y, dist):\n",
    "    \"\"\"\n",
    "    Computes Dynamic Time Warping (DTW) of two sequences.\n",
    "\n",
    "    :param array x: N1*M array\n",
    "    :param array y: N2*M array\n",
    "    :param func dist: distance used as cost measure\n",
    "\n",
    "    Returns the minimum distance, the cost matrix, the accumulated cost matrix, and the wrap path.\n",
    "    \"\"\"\n",
    "    assert len(x)\n",
    "    assert len(y)\n",
    "    r, c = len(x), len(y)\n",
    "    D0 = zeros((r + 1, c + 1))\n",
    "    D0[0, 1:] = inf\n",
    "    D0[1:, 0] = inf\n",
    "    D1 = D0[1:, 1:] # view\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            D1[i, j] = dist(x[i], y[j])\n",
    "    C = D1.copy()\n",
    "    for i in range(r):\n",
    "        for j in range(c):\n",
    "            D1[i, j] += min(D0[i, j], D0[i, j+1], D0[i+1, j])\n",
    "    if len(x)==1:\n",
    "        path = zeros(len(y)), range(len(y))\n",
    "    elif len(y) == 1:\n",
    "        path = range(len(x)), zeros(len(x))\n",
    "    else:\n",
    "        path = _traceback(D0)\n",
    "    return (D1[-1, -1] / sum(D1.shape), C, D1, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from nltk.metrics.distance import edit_distance\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "\n",
    "#dist_fun = euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "131\n",
      "183\n",
      "124\n",
      "30 25 30\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "n_frames_1 = []\n",
    "n_frames_2 = []\n",
    "n_frames_3 = []\n",
    "\n",
    "# Get the dimensions of all the frames for all classes\n",
    "for i in range(0,57):\n",
    "    n_frames_1.append(X1[i].shape[0])\n",
    "    n_frames_2.append(X2[i].shape[0])\n",
    "    n_frames_3.append(X3[i].shape[0])\n",
    "    \n",
    "# Find the maximum dimensions\n",
    "print(np.max(np.array(n_frames_1)))\n",
    "print(np.max(np.array(n_frames_2)))\n",
    "print(np.max(np.array(n_frames_3)))\n",
    "\n",
    "# Find the maximum dimension common to all the three classes\n",
    "size = []\n",
    "for each in n_frames_1:\n",
    "    if each in n_frames_2 and each in n_frames_3:\n",
    "        size.append(each)\n",
    "\n",
    "# max(size) will give you the maximum dimension of the frames which is common to all the classes\n",
    "print(np.max(size))\n",
    "\n",
    "m = np.max(size)\n",
    "# Find the index of the mfcc feature which has 124 frames\n",
    "p, q, r = n_frames_1.index(m), n_frames_2.index(m), n_frames_3.index(m)\n",
    "print(p,q,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set those mfcc features as your refrence in your DTW algo\n",
    "ref_1 = X1[p]\n",
    "ref_2 = X2[q]\n",
    "ref_3 = X3[r]\n",
    "\n",
    "# For each utterance in each class..do DTW with their reference utterance and find the path\n",
    "X = []\n",
    "Y = []\n",
    "c = 38\n",
    "# for Class 1\n",
    "for i in range(0, 57):\n",
    "    test = X1[i]\n",
    "    dist_fun = lambda template, test: np.linalg.norm(template - test, ord=1)\n",
    "    dist, cost, acc, path = dtw(ref_1, test, dist_fun)\n",
    "    tmp = path[1]\n",
    "    \n",
    "    fixed_mfcc = np.zeros((m, c))\n",
    "    for j in range(0, m):\n",
    "        fixed_mfcc[j] = test[tmp[j]]\n",
    "    X.append(fixed_mfcc)\n",
    "    Y.append(1)\n",
    "\n",
    "# for Class 2\n",
    "for i in range(0, 57):\n",
    "    test = X2[i]\n",
    "    dist_fun = lambda template, test: np.linalg.norm(template - test, ord=1)\n",
    "    dist, cost, acc, path = dtw(ref_2, test, dist_fun)\n",
    "    tmp = path[1]\n",
    "    \n",
    "    fixed_mfcc = np.zeros((m, c))\n",
    "    for j in range(0, m):\n",
    "        fixed_mfcc[j] = test[tmp[j]]\n",
    "    X.append(fixed_mfcc)\n",
    "    Y.append(2)\n",
    "\n",
    "# for Class 3\n",
    "for i in range(0, 57):\n",
    "    test = X3[i]\n",
    "    dist_fun = lambda template, test: np.linalg.norm(template - test, ord=1)\n",
    "    dist, cost, acc, path = dtw(ref_3, test, dist_fun)\n",
    "    tmp = path[1]\n",
    "    \n",
    "    fixed_mfcc = np.zeros((m, c))\n",
    "    for j in range(0, m):\n",
    "        fixed_mfcc[j] = test[tmp[j]]        \n",
    "    X.append(fixed_mfcc)\n",
    "    Y.append(3)\n",
    "\n",
    "    \n",
    "# X will contain the fixed length utterances for each class\n",
    "# Y will contain the actual class labels..like which utterance belongs to which class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some reshaping is rquired...basically each utterance is of dimensions 124x38...and we are reshaping it into 1x4712\n",
    "X = np.array(X).reshape(-1, m*c)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now split this fixed length data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the classifier using different kernel functions\n",
    "clf1 =svm.SVC(kernel= 'linear', probability=True)  # Linear Kernel\n",
    "clf2 =svm.SVC(kernel= 'poly', probability=True)  # Linear Kernel\n",
    "clf3 =svm.SVC(kernel= 'rbf', probability=True)  # Linear Kernel\n",
    "clf4 =svm.SVC(kernel= 'sigmoid', probability=True)  # Linear Kernel\n",
    "\n",
    "# Fit the classifier on the data\n",
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "clf3.fit(X_train, y_train)\n",
    "clf4.fit(X_train, y_train)\n",
    "\n",
    "# Do predictions\n",
    "y_pred_1=clf1.predict(X_test)\n",
    "y_pred_2=clf2.predict(X_test)\n",
    "y_pred_3=clf3.predict(X_test)\n",
    "y_pred_4=clf4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scores for plotting ROC and DET curves\n",
    "scores1 = clf1.predict_proba(X_test)\n",
    "scores2 = clf2.predict_proba(X_test)\n",
    "scores3 = clf3.predict_proba(X_test)\n",
    "scores4 = clf4.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.982456140351\n",
      "1.0\n",
      "0.964912280702\n"
     ]
    }
   ],
   "source": [
    "y_test = y_test.reshape(57, 1)\n",
    "\n",
    "y_pred1 = y_pred_1.reshape(57, 1)\n",
    "y_pred2 = y_pred_2.reshape(57, 1)\n",
    "y_pred3 = y_pred_3.reshape(57, 1)\n",
    "y_pred4 = y_pred_4.reshape(57, 1)\n",
    "\n",
    "scores1 = np.append(scores1, y_pred1, axis=1)\n",
    "scores1 = np.append(scores1, y_test, axis=1)\n",
    "np.savetxt('scores_linear_kernel', scores1)\n",
    "\n",
    "scores2 = np.append(scores2, y_pred2, axis=1)\n",
    "scores2 = np.append(scores2, y_test, axis=1)\n",
    "np.savetxt('scores_poly_kernel', scores2)\n",
    "\n",
    "scores3 = np.append(scores3, y_pred3, axis=1)\n",
    "scores3 = np.append(scores3, y_test, axis=1)\n",
    "np.savetxt('scores_rbf_kernel', scores3)\n",
    "\n",
    "scores4 = np.append(scores4, y_pred4, axis=1)\n",
    "scores4 = np.append(scores4, y_test, axis=1)\n",
    "np.savetxt('scores_sigmoid_kernel', scores4)\n",
    "\n",
    "# Accuracy\n",
    "print(clf1.score(X_test, y_test))\n",
    "print(clf2.score(X_test, y_test))\n",
    "print(clf3.score(X_test, y_test))\n",
    "print(clf4.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 13 16]\n",
      "[18 12 16]\n",
      "[26 24 32]\n",
      "[15 10 18]\n",
      "42\n",
      "46\n",
      "82\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "# Number of support vectors from each class\n",
    "print(clf1.n_support_)\n",
    "print(clf2.n_support_)\n",
    "print(clf3.n_support_)\n",
    "print(clf4.n_support_)\n",
    "\n",
    "# Indices of the support vectors in the data X\n",
    "print(len(clf1.support_))\n",
    "print(len(clf2.support_))\n",
    "print(len(clf3.support_))\n",
    "print(len(clf4.support_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
